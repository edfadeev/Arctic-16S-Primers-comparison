# set variables, program and script locations
FILELOCATION="/scratch2/efadeev/Primers_comp/SWARM_V3V4/with_ice"
NSAMPLE="50"

# set variables, program and script locations

SINA_PT="/scratch2/db/SILVA/SILVA_132_SSURef_NR99_13_12_17_opt.arb"

# scripts
CLIP_qsub="/scratch2/efadeev/aphros/ampliconNGS/clipping_aphros.sh"
TRIM_qsub="/scratch2/efadeev/aphros/ampliconNGS/trimming_aphros.sh"
MERGE_qsub="/scratch2/efadeev/aphros/ampliconNGS/merging_aphros.sh"
FASTQC_qsub="/scratch2/efadeev/aphros/ampliconNGS/fastqc_aphros.sh"
REFORMAT_qsub="/scratch2/efadeev/aphros/ampliconNGS/reformat_aphros.sh"
DEREPLICATE_qsub="/scratch2/efadeev/aphros/ampliconNGS/dereplicate_aphros.sh"
AMPLICONTAB="/scratch2/efadeev/aphros/ampliconNGS/amplicon_contingency_table.py" #also provided with swarm
SWARM_qsub="/scratch2/efadeev/aphros/ampliconNGS/swarming_aphros.sh"
MED_qsub="/scratch2/efadeev/aphros/ampliconNGS/med_aphros.sh"
REMOVE_SINGLE="/scratch2/efadeev/aphros/ampliconNGS/remove_singletons.R"
SINA_qsub="/scratch2/efadeev/aphros/TaxClassification/sina_aphros.sh" #this script is located in aphros/TaxClassification
CHECK_LCA="/scratch2/efadeev/aphros/ampliconNGS/find_missing_lca.py"

#modules
module load anaconda2 pear fastqc java bbmap swarm asplit R sina


###step 0: shorten file names
# save original file names
ls -1v ./Original/*R1_001.fastq > ./originalR1
ls -1v ./Original/*R2_001.fastq > ./originalR2

mkdir Renamed

# copy original files to new file names
new=1
for i in $(ls -1v ./Original/*R1_001.fastq)
do
  cp ${i} ./Renamed/${new}"_R1.fastq"
  ((new++))
done

new=1
for i in $(ls -1v ./Original/*R2_001.fastq)
do
  cp ${i} ./Renamed/${new}"_R2.fastq"
  ((new++))
done

# check that the renaming was done correctly
ls -1v ./Renamed/[0-9]*_R1.fastq > ./renamedR1
ls -1v ./Renamed/[0-9]*_R2.fastq > ./renamedR2

paste ./originalR1 ./renamedR1 > ./fileID_R1
paste ./originalR2 ./renamedR2 > ./fileID_R2

#the following commands schould not give any output
while read line ; do
  diff $(echo "$line")
done < ./fileID_R1

while read line ; do
  diff $(echo "$line")
done < ./fileID_R2


# create directory for qsub output files
mkdir Logfiles

###step 1: primer clipping 
#==FORWARD==
#Bact341F:
#5'-CCTACGGGNGGCWGCAG-3'         S-D-Bact-0341-b-S-17
#and Bact341F plus up to 3 additional bases in front:
#
#  5'-TCCTACGGGNGGCWGCAG-3'
# 5'-ATCCTACGGGNGGCWGCAG-3'
#5'-TGTCCTACGGGNGGCWGCAG-3'
#==REVERSE==
#Bact785R:
#5'-GACTACHVGGGTATCTAATCC-3'     S-D-Bact-0785-a-A-21

# bacterial primer V3-V4
FBC=CCTACGGGNGGCWGCAG # forward primer not anchored due to the mixed versions
RBC=^GACTACHVGGGTATCTAATCC # reverse primer
OFWD=16 # length of forward primer (17) - 1
OREV=20 # length of reverse primer (21) - 1
ERROR=0.16

mkdir Clipped

qsub -d ${FILELOCATION} -t 1-${NSAMPLE} -o ${FILELOCATION}/Logfiles -e ${FILELOCATION}/Logfiles -v FBC=${FBC},RBC=${RBC},OFWD=${OFWD},OREV=${OREV},ERROR=${ERROR} ${CLIP_qsub}

# cleaning up directories
mkdir ./Clipped/Clipped_logs
mv ./Clipped/*.log ./Clipped/Clipped_logs/
mv ./Clipped/*.info ./Clipped/Clipped_logs/


###step 2: quality trimming 
# creating directory for output of quality trimming
mkdir Trimmed

qsub -d ${FILELOCATION} -t 1-${NSAMPLE} -o ${FILELOCATION}/Logfiles -e ${FILELOCATION}/Logfiles ${TRIM_qsub}


# cleaning up directories
mkdir ./Trimmed/Trimmed_logs
mv ./Trimmed/*.log ./Trimmed/Trimmed_logs



###step 3: read merging
#creating directory for output of merging
mkdir Merged

qsub -d ${FILELOCATION} -t 1-${NSAMPLE} -o ${FILELOCATION}/Logfiles -e ${FILELOCATION}/Logfiles ${MERGE_qsub}

# cleaning up directories
mkdir ./Merged/Merged_logs
mv ./Merged/*.log ./Merged/Merged_logs

###step 4: quality control with FASTQC
# create directory for fastqc output
mkdir FastQC

qsub -d ${FILELOCATION} -t 1-${NSAMPLE} -o ${FILELOCATION}/Logfiles -e ${FILELOCATION}/Logfiles ${FASTQC_qsub}

# combine flags of 'Per base sequence quality' module for all files
grep "Per base sequence quality" ./FastQC/[0-9]*.assembled_fastqc/summary.txt > ./FastQC/QC_summary.txt

# range of read lengths
grep "Sequence length" ./FastQC/[0-9]*.assembled_fastqc/fastqc_data.txt > ./FastQC/QC_read_length.txt

# combine flags of 'Sequence Length Distribution' module for all files including most abundant read lengths
for i in $(seq 1 ${NSAMPLE})
do
  awk '/^>>Sequence Length Distribution/,/^>>END_MODULE/' ./FastQC/$i".assembled_fastqc"/fastqc_data.txt |\
  sed -e '1,2d' -e '$d' > ./FastQC/$i".assembled_fastqc"/fastqc_SLD.txt

  sort -t$'\t' -k2nr ./FastQC/$i".assembled_fastqc"/fastqc_SLD.txt |\
  head -1 |\
  paste <(grep "Sequence Length Distribution" ./FastQC/$i".assembled_fastqc"/summary.txt) - 
done > ./FastQC/QC_read_distribution.txt

# count sequences
# only counting forward read as representative for PE
grep -c '^@' ./Renamed/[0-9]*_R1.fastq >> nSeqs_raw_V3V4_all.txt
grep -c '^@' ./Clipped/[0-9]*_clip_R1.fastq >> nSeqs_clip_V3V4_all.txt
grep -c '^@' ./Trimmed/[0-9]*_ptrim_R1.fastq >> nSeqs_trim_V3V4_all.txt
grep -c '^@' ./Merged/[0-9]*.assembled.fastq >> nSeqs_merge_V3V4_all.txt

#put all the numbers in one table
paste nSeqs_raw_V3V4_all.txt nSeqs_clip_V3V4_all.txt nSeqs_trim_V3V4_all.txt nSeqs_merge_V3V4_all.txt > nSeqs_V3V4_all.txt

###step 5: swarm OTU clustering 

# create directory for swarm input and output
mkdir Swarm
#extracting FASTA
qsub -d ${FILELOCATION} -t 1-${NSAMPLE} -o ${FILELOCATION}/Logfiles -e ${FILELOCATION}/Logfiles ${REFORMAT_qsub}


#dereplication of samples
qsub -d ${FILELOCATION} -t 1-${NSAMPLE} -o ${FILELOCATION}/Logfiles -e ${FILELOCATION}/Logfiles ${DEREPLICATE_qsub}


# study level dereplication 

cd ./Swarm/

export LC_ALL=C
cat *_dereplicated.fasta | \
awk 'BEGIN {RS = ">" ; FS = "[_\n]"}
     {if (NR != 1) {abundances[$1] += $2 ; sequences[$1] = $3}}
     END {for (amplicon in sequences) {
         print ">" amplicon "_" abundances[amplicon] "_" sequences[amplicon]}}' | \
sort --temporary-directory=$(pwd) -t "_" -k2,2nr -k1.2,1d | \
sed -e 's/\_/\n/2' > all_samples.fasta

#building amplicon contingency table (use script from swarm 1.20)

python ${AMPLICONTAB} *_dereplicated.fasta > amplicons_table.csv

#swarming

qsub -d ${FILELOCATION}/Swarm -o ${FILELOCATION}/Logfiles -e ${FILELOCATION}/Logfiles ${SWARM_qsub}

# building OTU contingency table for multiple samples 
STATS="amplicons_stats.txt"
SWARMS="amplicons.swarms"
AMPLICON_TABLE="amplicons_table.csv"
OTU_TABLE="OTU_contingency_table.csv"

# Header
echo -e "OTU\t$(head -n 1 "${AMPLICON_TABLE}")" > "${OTU_TABLE}"

# Compute "per sample abundance" for each OTU
awk -v SWARM="${SWARMS}" -v TABLE="${AMPLICON_TABLE}"  'BEGIN {FS = " "
            while ((getline < SWARM) > 0) {

                swarms[$1] = $0
            }
            FS = "\t"
            while ((getline < TABLE) > 0) {
                table[$1] = $0
            }
           }

     {# Parse the stat file (OTUs sorted by decreasing abundance)
      seed = $3 "_" $4
      n = split(swarms[seed], OTU, "[ _]")
      for (i = 1; i < n; i = i + 2) {
          s = split(table[OTU[i]], abundances, "\t")
          for (j = 1; j < s; j++) {
              samples[j] += abundances[j+1]
          }
      }
      printf "%s\t%s", NR, $3
      for (j = 1; j < s; j++) {
          printf "\t%s", samples[j]
      }
     printf "\n"
     delete samples
}' "${STATS}" >> "${OTU_TABLE}"


###step 6: taxonomic classification

# convert lowercase sequences to uppercase sequences in amplicons_seeds.fasta
awk '{print /^>/ ? $0 : toupper($0)}' amplicons_seeds.fasta > amplicons_seeds_uc.fasta

#removal of singletons
# this will output the accession numbers of all non-singleton swarms 
# and a table with the precentage of retained sequences per sample if singleton swarms are removed
Rscript ${REMOVE_SINGLE} 
# rename the original amplicons_seeds_uc.fasta so that it is not overwritten
mv amplicons_seeds_uc.fasta amplicons_seeds_uc_all.fasta
# select only the representative sequences of non-singleton swarms 
grep -A1 -F -f heavy.accnos amplicons_seeds_uc_all.fasta | sed '/^--$/d' > amplicons_seeds_uc.fasta


cd ..


# splitting seed sequence fasta file for parallel processing
mkdir Sina
cd Sina
asplit '^>' 2000 < ../Swarm/amplicons_seeds_uc.fasta #split fasta file in fasta files with 2000 sequences each

# Determine how many chunks there are...
JOBCOUNT=$(ls -1 out* | wc -l) #specify the number of files in array job

qsub -d ${FILELOCATION}/Sina -t 1-${JOBCOUNT} -o ${FILELOCATION}/Logfiles -e ${FILELOCATION}/Logfiles -v SINA_PT=${SINA_PT} ${SINA_qsub}

# check that all sequences were classified
# no output is good, otherwise the sequence number per 2000-sequence package is printed to standard output
for file in ${FILELOCATION}/Logfiles/sina.e*
do 
  ${CHECK_LCA} $file
done

# Time to gather up the useful info from the split output... 
# In grep, -h suppresses printing of filenames for results

# Get all the swarm seed hashes (sort of like accessions)
grep -h '^sequence_identifier' $(ls -1v ${FILELOCATION}/Logfiles/sina.e*) | sed 's/^sequence_identifier: //' > amplicons_seeds.accnos

#check if the order is the same as in amplicons_seeds_uc.fasta
grep '^>' ../Swarm/amplicons_seeds_uc.fasta | sed 's/^>//' | diff - amplicons_seeds.accnos

# Get all corresponding taxonomic paths (note the same order as the accnos)
grep -h '^lca_tax_slv' $(ls -1v ${FILELOCATION}/Logfiles/sina.e*) | sed 's/^lca_tax_slv: //' > amplicons_seeds.tax_slv

# Get all alignment qualities (for filtering later)
grep -h '^align_quality_slv' $(ls -1v ${FILELOCATION}/Logfiles/sina.e*) | sed 's/^align_quality_slv: //' > amplicons_seeds.align_quality_slv

# merge these output files...
paste amplicons_seeds.accnos amplicons_seeds.align_quality_slv amplicons_seeds.tax_slv > amplicons_seeds_taxonomy.txt 
cd ..

# copy final output files to working directory
cp ./Swarm/OTU_contingency_table.csv ./
cp ./Sina/amplicons_seeds_taxonomy.txt ./


